pretrained: #/path/to/pretrained_model.ckpt
data:
    variables:
        # Set Data paths
        DATA_PATH: &data_path training_data/wlasl_new.json
        POSE_PATH: &pose_path training_data/wlasl_poses_pickle/
        TARGET_Path: &target_path  openhands.datasets.isolated.WLASLDataset
        
        # Set modality
        MOD: &modality "pose"

        # Set model name
        MODEL_NAME: &model_name base_bert

        # Select phonological parameters
        PARAMETERS: &params  [
                "Handshape"
                # "Selected Fingers",
                # "Flexion",
                # "Spread",
                # "Spread Change",
                # "Thumb Position",
                # "Thumb Contact",
                # "Sign Type",
                # "Path Movement",
                # "Repeated Movement",
                # "Major Location",
                # "Minor Location",
                # "Second Minor Location",
                # "Contact",
                # "Nondominant Handshape", 
                # "Wrist Twist",
                # "Handshape Morpheme 2"
            ]

    modality: *modality

    test_pipeline:
            dataset:
                _target_: *target_path
                split_file: *data_path
                root_dir: *pose_path
                splits: "test"
                modality: *modality
                inference_mode: true
                results: results/save_results.jsonl

            transforms:
                - PoseSelect:
                    preset: mediapipe_holistic_minimal_27
                - CenterAndScaleNormalize:
                    reference_points_preset: shoulder_mediapipe_holistic_minimal_27
                    scale_factor: 1

            dataloader:
                _target_: torch.utils.data.DataLoader
                batch_size: 32
                shuffle: false
                num_workers: 3
                pin_memory: true
                drop_last: false

            parameters: *params

model:
    encoder:
        type: pose-flattener
        params:
            num_points: 27
    decoder:
        type: param_bert
        params:
            max_position_embeddings: 121
            layer_norm_eps: 1e-12
            hidden_dropout_prob: 0.1
            hidden_size: 96
            num_attention_heads: 6
            num_hidden_layers: 3
            cls_token: true
            
        parameters: *params

optim:
    loss: 'CrossEntropyLoss'
    optimizer:
        name: Adam
        params:
            lr: 1e-4

    scheduler:
        name: CosineAnnealingLR
        params:
            last_epoch: -1
            T_max: 10

trainer:
    max_epochs: 150
    # resume_from_checkpoint: /path/to/model.ckpt

exp_manager:
    create_tensorboard_logger: true
    create_wandb_logger: true
    wandb_logger_kwargs:
        name: *model_name
        project: PASL

    create_checkpoint_callback: true
    checkpoint_callback_params:
        monitor: "val_acc"
        mode: "max"
        save_top_k: 1
        dirpath: "new_models/"

    early_stopping_callback: true
    early_stopping_params:
        monitor: "val_acc"
        patience: 80
        verbose: true
        mode: "max"